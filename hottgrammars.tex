\subsection{Ranta's HoTT Grammar} \label{rantaHott}

In 2014, Ranta gave an unpublished talk at the Stockholm Mathematics Seminar
\cite{aarneHott}. This project aimed to provide a translation like the one
desired in our current work, but it took a real piece of mathematics text as the
main influence on the design of the grammar.

Using a page of text from Peter Aczel's writings goes over a few
standard HoTT definitions and theorems, the grammar allows the translation of
the latex document in English to the same document in French, and to a pidgin
logical language. The central motivation of this grammar was to capture entirely
``real" natural language mathematics. Therefore, it isn't reminiscent of the
slender abstract syntax the type theorist adores, and sacrificed ``syntactic
completeness" for ``semantic adequacy". The abstract syntax is much larger and
very expressive, but is no longer easy to reason about. Additionally, certain
design choices feel ad-hoc. Another defect is that this grammar overgenerates
when parsing from the pidgin logical language. Producing a unique parse from the
PL side would require a significant amount of refactoring. While it is
presumably possible to carve a subset of the GF HoTT abstract file to
accommodate an Agda program, but one encounters rocks as soon as one begins to
dig.

In \autoref{fig:R1} one can see different syntactic presentations of a notion of
\emph{contractability}, that a space is deformable into a single point, or that
a Type is actually inhabited by a single unique term. Some rendered latex is
compared with the translated pidgin logic code (after refactoring of Ranta's
linearization scheme) and an Agda program. We see that it was fairly easy to get
the notation for our cubicalTT grammar \ref{cubic}. When parsing the logical
form, unfortunately, the grammar is incredibly ambiguous.

\input{latex/ContrClean}

To extend this grammar to accommodate a chapter worth of material, let alone a
book, will not just require extending the lexicon, but encountering other
syntactic phenomena that will further be difficult to compress when writing a
dual grammar for Agda's concrete syntax. This demonstrates that to design a
grammar prioritizing \emph{semantic adequacy} and subsequently trying to
incorporate \emph{syntactic completeness} is difficult, and probably not the
best grammar design choice.

The next grammar we present, taking an actual programming language parser in
BNFC, GFifying it, and trying to use the abstract syntax to model natural
language, gives in some sense a dual challenge, where the abstract syntax
remains simple as in our dependently typed grammar \ref{npf}, but its
linearizations may become increasingly complex, especially when generating
natural language.

\subsection{cubicalTT Grammar} \label{cubic}

Cubical type theories arose out of the desire to give a complete computational
interpretation to HoTT, whereby univalence would become a theorem rather than an
axiom \cite{cohen:hal-01378906}. The utility of this is that canonicity, the
property of an expression having a irreducible normal form, is satisfied for all
expressions - all natural number expressions must evaluate to numerals.
Univalence in classical HoTT, by introducing a type without computational
behavior, means that the constructivist using Agda will be able to define terms
which don't normalize.

Cubical Type Theories originated when looking beyond simplicial models of type
theory to cubical categories instead \cite{bezem2017univalence}, and gave a
blueprint for a totally new type theory which natively supports proving
functional extensionality, which is a especially important for mathematicians.
The ideas from cubical type theories cubical generated a series of proof
assistants: Cubical \cite{huberCub}, cubicalTT \cite{cubicaltt}, and Cubical
Agda \cite{cubicalAgda}, as well as other in originating from Robert Constables
disciples in the NuPrl tradition \cite{Angiuli_2018} \cite{redTT} \cite{coolTT}.
cubicalTT, had an unambiguous BNFC grammar which more or less represents a
kernel of Agda with cubical primitives. This final grammar, which we simply
denote as cubicalTT, took the actual cubicalTT grammar and GFified the subset
which is in the intersection with vanilla Agda. Extending our GF version to
include cubical primitives would facilitate the extension of the work to Cubical
Agda, and we hope future endeavors will go in this direction. Cubical Agda
supports Higher Inductive Types \cite{hits} natively and is capable of all types
of new constructions not mentioned in the HoTT book. It is also incredibly
experimental, with large changes to the standard library constantly underway as
can be seen in \ref{homo}.

\subsubsection{GFification}

Our grammar for vanilla dependent $\Pi$-types \ref{npf} was actually a subset of
the current cubicalTT abstract syntax. We give a brief sketch of the algorithm
to go between a BNFC grammar and a GF grammar. BNFC essentially combines the
abstract and concrete syntax, enabling a hierarchy of numbered expressions
\term{ExpN} to minimize use of parentheses. So, given $m$ names and choosing
$Name_i$, we take the accompanying BNFC rule :

$$Name_i.\; ReturnCat_{i_n} ::= s^0_{i}\;C^0_{i_0}\;...\;C^{n-1}_{i_{n-1}}\;s^n_{i}\;;$$

where string $s^i_j$ may be empty and the $k$ in the $i^{th}_k$ subscript represents the 
precedence number of a category. These precedences are indicated with a
\term{Coercions N} keyword in BNFC. We can produce the following in GF.

$$cat\; Name_i\; \bigcap\{ReturnCat_i,C^0,..., C^{n-1}\}\;;$$
$$fun\; Name_i\:{:} C^0 \rightarrow ... \rightarrow C^{n-1} \rightarrow ReturnCat_i $$
$$lincat \: \bigcap\{ReturnCat_i,C^0,..., C^{n-1}\}\;; = TermPrec$$
$$lin \; Name_i\;c^0\;... \;c^n = mkPrec(i_n,(s^0_{i}\texttt{++}usePrec(i_0+1,c^0)\texttt{++}...\texttt{++}usePrec(i_{n-1}+1,c^{n-1})\texttt{++}s^n_{i})) ;$$

where $c^i \in C^j \; \forall i,j$, and \term{usePrec} and \term{mkPrec} come
from the RGL, as seen earlier. We also note that some \term{lincat}s can be
arbitrary linearization types, for it is only when a precedence
is observed that the \codeword{TermPrec} is applicable. The use of
\term{usePrec} is only applicable when $i_k$ isn't empty. Additionally, this
doesn't account for the fact that already some categories may have been
witnessed in which case we want to intersect over the whole set of rules at
once. We reiterate the examples from the simply typed lambda calculus. The BNFC
code results in the GF code immediately below.

\begin{verbatim}
--BNFC
Lam. Exp  ::= "\\" [PTele] "->" Exp ;
Fun. Exp1 ::= Exp2 "->" Exp1 ;
-- GF
cat Exp ; PTele ;
fun
  Lam : [PTele] -> Exp -> Exp ;
  Fun : Exp -> Exp -> Exp ;
lincat Exp = TermPrec ; [PTele] = Str ;
lin 
  Lam pt e = mkPrec 0 ("\\" ++ pt ++ "->" ++ usePrec 0 e) ;
  Fun = mkPrec 1 (usePrec 2 x ++ "->" ++ usePrec 1 y) ;
\end{verbatim}

This more or less elaborates exactly how to implement a programming language
with unambiguous parsing in GF. There is also a simple means of translating
lists, including BNFC's \term{separator} and \term{terminator} keywords during
the linearization process. Finally, there is a custom \term{token} keyword, and
this is perhaps the most important feature absent in GF. Because BNFC generates
Haskell code similar to GF, it would also be possible to translate the trees
directly, if parsing complexity with GF was found to be slower than BNFC.

Most interesting is to observe what GF has but is absent in BNFC, namely, the
ability to add records and paremeters into the linearization types generally.
One could add unique categories in GF $Exp_1,...,Exp_n$, but this would clutter
the abstract syntax with information which isn't \emph{semantically} relevant.
And while the Haskell code generated by BNFC for cubicalTT is sent through a
resolver to the \emph{actual} abstract syntax used by the type-checker and
evaluator, the fact that it parses the concrete syntax into an appropriate
intermediary form is enough for our purposes. The grammar is available at
\cite{warrickCub}.

\subsubsection{Difficulties}

While cubicalTT gives unique parses a PL, linearizing to a CNL for mathematics was not implemented due to
time constraints, and the difficulties already encountered for an even simpler
programming language \ref{assoc}, namely that types and terms in dependent type
theory can be of just about any grammatical category. We list a few examples :

\begin{itemize}
\item nouns, ``zero"
\item adjectives, ``prime"
\item verbs, ``add"
\item verb phrase, ``apply the function to the subset of..."
\item sentence, ``if x is odd, then y is even"
\item paragraph or more, ``suppose x. then by y we know z. hence, w. but ..."
\end{itemize}

In \cite{rantaZ}, the authors, generating human readable natural language from
specifications, used a word type with many different fields for different
grammatical categories (with the same grammatical categories sometimes
accounting for multiple fields), in addition to symbolic fields. While deemed
successful by the client, it would be interesting to apply this methodology to
cubicalTT the grammar, and see how it scales once one begins to add more of
Agda's capabilities. Their system also involved other components like Haskell
transformations, and it is uncertain how these specific approaches would also
allow for the generation of more \emph{semantically adequate} mathematical
language.

Other issues encountered in this grammar were Agda's pattern matching, whereby
arguments are arranged in a matrix, as opposed to explicit cases, or \emph{splits}.
We define \codeword{equalNat} in cubicalTT as:

\begin{verbatim}
equalNat : nat -> nat -> bool = split
    zero -> split@ ( nat -> bool ) with
      zero  -> true
      suc n -> false
    suc m -> split@ ( nat -> bool ) with
      zero  -> false
      suc n -> equalNat m n
\end{verbatim}

The problem is that when linearizing a split, one cannot know how many further
splits will take place, and so going from this form to the more ``readable" Agda
code below is outside of GF's linearization capabilities - although a proof of
this fact would require advanced mathematical capabilities. One could instead
just a new form of declarations in the abstract syntax, but this would require
more Haskell overhead to allow for the correct AST transformations.

\begin{verbatim}
equalNat : nat → nat → bool
equalNat zero zero = true ; 
equalNat zero (suc n2) = false ;
equalNat (suc n1) zero = false ;
equalNat (suc n1) (suc n2) = equalNat n1 n2
\end{verbatim}

The way lists are dealt with in natural language versus programming
languages also present obstacles, because the RGL's support for lists require $2$ 
numbers of categories in the end node whereas our Agda
grammar may instead have \codeword{cat[1]} or \codeword{cat[0]} for the same
category. Resolving this overloading of categories for the two linearization
spaces will also require Haskell transformations. 

\subsubsection{More advanced Agda features}

Our grammar covers just a small kernel of Agda's features and
syntax. Aside from telescopes, other syntactic sugar features
of Agda include unicode support, do notation, idiom brackets, generalized
variable declarations, and more. While require significant work to extend the 
cubicalTT grammar with these, it is doubtful
these kinds of features offer significant theoretical challenges in terms of
translation to natural language.

From the semantic side, however, Agda offers many features which extend just the
kernel of the $\Pi$, $\Sigma$, and recursive data type definitions which form
the basis of any dependent type theory. These include universes, sized types,
modules, overloading for more ad-hoc polymorphism, proof by reflection, a sort
system, higher inductive types (only in Cubical Agda), and many more things
visible in the Agda documentation \cite{agdaDocs}. Additionally, it has more
traditional PL features, like the ability to perform side effects or call
Haskell functions. Adding any one of these not only adds overhead to the parser,
but would require lots of thought in terms of how to these features manifest in
natural language for mathematicians (and programmers). Additionally, these
features make the metatheory of Agda much more expensive to understand, in
addition to the practical implications of introducing bugs in its
implementation.

Mathematics on the other hand, doesn't often introduce more advanced ``semantic
machinery" like those Agda features just listed. Perhaps idioms and conventions
change, as well as general machinery like category theory offer ways of presenting
ideas more succinctly, but these are merely reflected in the presentation, not
in the underlying logical formalism. The linguistic evolution of mathematics
additionally reflects some kind of meta-changes, but not in a coherent way that
is documented or even understood. For many mathematicians are largely interested
in proving theorems and solving problems specific to some domain. The resolution
of these meta-ideas from both the type theoretic and mathematical perspectives
is what makes this problem of translation so philosophically intriguing, as well
as even more intractable.

\subsection{Comparing the Grammars}

We compare the Ranta's HoTT grammar and our cubicalTT grammar, with a focus on
comparing syntactic completeness and semantic adequacy.

Our cubicalTT grammar takes expressions as its epicenter, whereby
declarations, branches, telescopes, ``where expressions", etc. offer syntactic
sugar so that it becomes a minimally readable programming language. It is a
synthetic approach to writing a grammar, whereby one has an \emph{a priori} idea of
what an expression syntactically should be, with the most important feature
being that it is inductively generated. It is not really concerned with
semantics per-se, because this is the job of the type-checker and evaluator.

Ranta's HoTT grammar on the other hand, analyses real text, and decisions about
the grammar are made posterior to observing phenomena in the text being
analyzed. The grammar makes distinction between \codeword{Formulas.gf}, namely
expressions with symbolic support for latex, \codeword{Framework.gf} which allows
one to construct natural language sentences, and a \codeword{HottLexicon.gf}. This
grammar, while having some inductive notion of what an expression is, puts the
bulk of work in producing valid sentences in \codeword{Framework.gf}.

\begin{verbatim}
cat
  Paragraph ;        -- definition, theorem, etc
  Definition ;       -- definition of a new concept
  Assumption ;       -- assumption in a proof  
  [Assumption]{1} ;  -- list of assumptions in one sentence 
  Conclusion ;       -- conclusion in a proof 
  Prop ;             -- proposition (sentence or formula) 
  Sort ;             -- set, type, etc corresponding to a common noun
  Ind ;              -- individual element, a singular term
  Fun ;              -- function with individual value
  Pred ;             -- predicate: function with proposition value 
  [Ind] ;            -- list of individual expressions  
  UnivPhrase ;       -- universal noun phrase          
  ConclusionPhrase ; -- conclusion word               
  Label ;            -- name/number of definition, theorem, etc 
  Title ;            -- title for theorem, definition, etc
\end{verbatim}

The distinction between individuals, propositions, sorts, functions, and
predicates also allows more nuance, but delegates the work of deciding what
category a term represents much more difficult, in addition to complicating the
possibility of having some algorithm infer the right category. The expressions,
\codeword{Exp}, can be embedded into any of these categories. Additionally, we
see that the universal phrase, the notion of a $\Pi$-type, merits semantic
distinction in this grammar, with unique functions being assigned for all the
(observed) ways of saying it - this is the case with existential statements as
well.

\begin{verbatim}
  plainUnivPhrase   : [Var] -> Sort -> UnivPhrase ;--for x, y : A
  eachUnivPhrase    : [Var] -> Sort -> UnivPhrase ;--for each x,y : A
  allUnivPhrase     : [Var] -> Sort -> UnivPhrase ;--for all x,y : A
  ifUnivPhrase      : [Var] -> Sort -> UnivPhrase ;--if x,y : A
  if_thenUnivPhrase : [Var] -> Sort -> UnivPhrase ;--if x,y : A then
\end{verbatim}

One caveat is that set comprehensions are treated as expressions, whereas
existential phrases are propositions, even though to the Agda programmer they
are the \emph{same thing}. This differences arises in the fact that expressions
are meant to be symbolic in this grammar, whereas functions taking \term{Exp}
arguments generally return things with grammatical categories with possibly
auxiliary data, i.e.

\begin{verbatim}
lincat
  Sort = SortExp ;
  Fun  = FunExp ;
  Ind  = IndExp ; 
  Prop = S ;
oper
  SortExp = {cn : CN ; postname : Str ; isSymbolic : Bool} ;
  IndExp = {s : NP ; isSymbolic : Bool} ;
  FunExp = {s : CN ; isSymbolic : Bool} ;
\end{verbatim}

Ranta chose to prioritize semantic adequacy by placing the manifold
grammatical categories at the forefront. This was not an error, as Peter Aczel's
writing mixed notations from set theory, type theory, first order logic, and
homotopy theory. For as much as the type theorist insists on her exclusive use
of types, the written language tradition is still tied to the logical and set
theoretic tradition of presenting mathematics - this results in a
more expressive abstract syntax.

This includes document structure categories, \codeword{Title}, \codeword{Label},
\codeword{Paragraph}, \codeword{Definition}, \codeword{Conclusion}, etc. While
these may resembling a module system in ways, they also reflect a different
semantic sense than Agda's module system, which gives the programmer greater
control of handling software complexity. \codeword{ConclusionPhrase} reflects
what Agda's typechecker infers and is displayed to the user, and is therefore
redundant from the programmers perspective.

Another observation about Ranta's grammar is that the certain notions come with
more semantic information that the type-checker would be able to infer, so for
instance, \codeword{fiberExp} is a binary function, as opposed to the cubicalTT
grammar which treats it as a variable. This we may remember, leads to the
``application hell" observed earlier \ref{appHell}.

Despite the complexity of the abstract syntax relative to cubicalTT, it is
remarkable that Ranta was able to capture the entire text with a few days of
labor. Expertise in GF, however, reveals itself through trial, error, and
patience. Despite the success, we hypothesize that extending it to longer
lengths of text would very difficult for anyone without deep knowledge of GF and
type theory generally. The ease of extending cubicalTT to cover more text,
despite its limitations regarding language generation, poses a dual problem of
how to extending the concrete syntax each time a new grammatical ``feature" is
discovered. We have included the text parsed by Ranta's HoTT grammar implemented
both an Agda representation which type-checks, as well as the cubicalTT syntax
for these terms, in the appendix \ref{comparison}.

\subsubsection{Ideas for resolution}

Based off these comparisons, we now propose a road-map for future investigations
of how to build a ``master grammar", which should ideally seek to do at least
the following:

\begin{itemize}
\item Allow for expressive natural language - maximize \emph{semantic adequacy}
\item Enable parsing of a real programming language - ensure \emph{syntactic completeness}
\item Allow GF developers to expand the grammar in a compositional, modular, safe, reliable,
  and methodologically precise way
\item Enable long-term integration of the grammars into practical tools for
  mathematicians and computer scientists
\end{itemize}

We therefore believe there is a set of principles one can follow to achieve
these goals : namely, start with a small, syntactically precise core, and extend
it based off the needs of either the programmer or the mathematician.

Let's suppose that our hypothetical ``core" should consist of a desugared type
theory with $\Pi$, $\Sigma$, and Equality types, with their respective
introduction and elimination forms, inductive definitions and a means of case
analysis, and declarations for building types and terms. We could then
\emph{extend} this with telescopes for syntactic sugar, ``where" and ``let"
bindings to allow for local definitions, and modules to allow for the basic
needs of a suitable programming language - and we'd essentially have the
cubicalTT grammar. One thing to be emphasized is that the extension should
already map to the core. As was noted when we discussed Haskell transformations
for Ranta's logic grammar \ref{cade}, the mapping $\llbracket - \rrbracket :
Extended \to Core$ can follow relatively conventional techniques.

This can then be extended again to include more nuance that a particular Agda programmer
might desire : unicode support, universes, Agda-style pattern matching, cubical
primitives (although this \emph{fundamentally} changes the underlying type
theory), higher inductive types, and more. It should be noted that creating a GF
grammar capable of parsing all of Agda would be overkill, and working with
Agda's existing parser would probably be preferred at some point if for no other
reason than that the myriad of features would create a grammar that would no
longer be feasible for natural language generation.

Once the grammar for the logical framework has been established, the grammar
writer would then have the lexical data, specific to the domain being modeled -
our two case studies previous being natural numbers propositions and
notions from homotopy type theory.  This presents the challenge of how ``deep"
does one wish to embed the domain into GF. For our cubicalTT parser, we chose
the shallowest possible embedding, whereby every term was just a \emph{variable}
with no semantic distinction. In the grammar for QA, we chose the deepest
possible embedding, with \term{Nat} being a distinguished category, not just
a function. While this is convenient for the example, it was only so because
we could coerce GFs builtin number to, i.e. \codword{Int -> Nat}. Unless one
intends to use GF's dependent types, this deep embedding is likely unnecessary,
and in some sense creates too much semantic space in the grammar.

The ``in-between" depth is to include \term{Nat} as an expression, whereby the
zero, successor, and induction principle, included as functions, retain their
arities from the actual programming language, but don't actually specify what
types of expressions work for them - this work is delegated to the type-checker.
While this has the benefit of disallowing the ``application hell" we saw in
\ref{appHell}, it also requires what we'll passively call ``arity inference",
and therefore some components of the type-checker would be needed to scale this
approach. Additionally, the use of the phrase ``successor function" to refer to
$\eta$-expanded form in contrast to ``the successor of" reveals the deep
difficulties of how to delegate unique linguistic forms to all the possible
arity assignments, something that a programming language can infer automatically
based of a term's use.

Once the grammar has been extended \emph{enough}, satisfying the programmer's
needs, one would encounter the even more difficult task of pleasing the
mathematician. One could have categories for things like sets and propositions,
as Ranta does in the HoTT grammar. These extensions of the
$\{\Pi,\Sigma,\equiv,...\}$ core, if given a Haskell embedding, could be made
such that any proposition in the extended langauge would be mapped to its type
in the core language. In Ranta's code, the existential propositions and set
comprehension syntax could be evaluated, via Haskell, to the $\Sigma$ type in
the core grammar, thereby allowing for a translation from semantically sound
utterances to syntactically complete ones.

We offer a conterexample, namely the definition of a left coset in group theory,
$gH = \{gh {:}h \in H\}\; \forall g \in G$ , because $h \in H$ is a judgment and
not a type. Indeed, quotients, subsets, and subgroups in type theory must be
treated differently than their set-theoretic counterparts. The sets must be
given clever encodings which don't precisely match their set theoretic
intuition, especially with respect to syntax \cite{zipperer2016formalization}.
Additionally, in the reverse direction, taking a $\Sigma$ type and generating a
natural language utterance which may be of the type, set, or one the many
propositional flavors would require some pragmatic knowledge that our system is
not capable of handling.

Our proposal is to build a core, \emph{syntactically complete} grammar similar
to cubicalTT and extend it to a \emph{semantically adequate} which follows
Ranta's HoTT grammar methodology. It should be based of the condition that they
are coherent in a sense that the extended grammar can be compositionally
evaluated to the core, via a Haskell function, following Ranta's lead in his
Cade grammar.

\paragraph{Typechecker's Role}

One of the central pieces of a programming language missing from this approach
is the role of the type-checker. In dependently typed languages like Agda, where
the type-checker evaluates programs in types to a canonical form, this is
especially acute - for the typechecker tells you when a proof is
\emph{syntactically valid}. For the mathematician, a proof may be valid even if
it doesn't type-check, because they can account for many details which ensure
that an argument is articulated honestly, or at least as honestly as possible.
While there may be small errors, presumptions, or holes in a syntactic proof,
this ultimately doesn't detract from the \emph{semantic ideas} being portrayed
and perceived.

The coherence of a semantically adquate and syntactically complete object via an
AST sadly doesn't seem feasible without some kind of intermediary verification
procedure. We imagine that a semantically adequate proof, when being translated
through some idealized system, could produce an Agda proof with holes, for
instance. These could either be filled in with tactics or with help through
Agda's interactive proof development system.

This should be a long term goal for whoever continues this project. Even if
there's not an equivalence between syntactically complete and semantically
adequate objects, it is feasible that one can come up with ways of approximating
one inside the other's domain, and we believe the power of dependent type
theories may give us one way of achieving this approximation.
