\section{Prior GF Formalizations}

\subsection{CADE 2011} \label{cade}

In \cite{rantaLog}, Ranta designed a grammar which allowed for predicate logic
with a domain specific lexicon supporting mathematical theories like geometry or
arithmetic. The syntax was both meant to be relatively complete so that typical
logical utterances of interest could be accommodated, as well as support
relatively non-trivial linguistic nuance. The nuances included lists of terms,
predicates, and propositions and in-situ and bounded quantification. The more
interesting syntactic details captured in this work was by means of an extended
grammar on top of the core. The bidirectional transformation between the core
and extended grammars via a Haskell transformation also show the viability and
necessity of using more expressive programming languages as intermediaries when
doing thorough translations.

As a simple example, the proposition $\forall x (Nat(x) \supset
Even(x) \lor Odd(x))$ can be given a maximized and minimized version. The tree
representing the syntactically complete phrase ``for all natural numbers
x, x is even or x is odd" would be minimized to a tree which linearizes to the
semantically adequate phrase ``every natural number is even or odd".

We see that our criteria of semantic adequacy and syntactic completeness can
both occur in the same grammar, with Haskell level transformation allowing one
to go between the them. Problematically, this syntactically complete phrase
produces four ASTs, with the ``or" and ``forall" competing for precedence. There
is therefore no assurance give the user of the grammar confidence that her
phrase was correctly interpreted without deciding which translation is best.

In the opposite direction, the desugaring of a logically ``informal" statement
into something less linguistically idiomatic is also accomplished. Ranta claims
``Finding extended syntax equivalents for core syntax trees is trickier than the
opposite direction". While this may be true for this particular grammar, we
argue that this may not hold generally.


The RGL supports listing the sentences, noun phrases, and other grammatical
categories. One can then use Haskell to unroll the lists into binary operators, or
alternatively transform them in the opposite direction. , we first mention that
GF natively supports list categories, the judgment \term{cat [C] {n}} can be
desugared to

\begin{verbatim}
  cat ListC ;
  fun BaseC : C -> ... -> C -> ListC ; -- n C â€™s
  fun ConsC : C -> ListC -> ListC
\end{verbatim}

We could therefore transform the extended language phrase ``The sum of x , y ,
and z is equal to itself" in to core language phrase ``the sum of the sum of x
and y and z is equal to the sum of x and the sum of y and z". Parsing this core
string gives 32 unique trees, and dealing with ambiguities must be solved first
and foremost to satisfy the PL designer who only accepts unambiguous parses.

Ranta outlines the mapping, $\llbracket - \rrbracket : Core \to Extended$,
which should hypothetically return a set of extended sentences for a more
comprehensive grammar.

\begin{itemize}
\item Flattening a list
  $x\ and\ y\ and\ z\ \mapsto x,\ y\ and\ z$
\item Aggregation
  $x\ is\ even\ or\ x\ is\ odd\ \mapsto x\ is\ even\ or\ odd$
\item In-situ quantification \\
  $\forall\ n\ \in Nat,\ x\ is\ even\ or\ x\ is\ odd \mapsto every\ Nat\ is\ even\ or\ odd$
\item Negation
  $it\ is\ not\ that\ case\ that\ x\ is\ even\ \mapsto \x is\ not\ even$
\item Reflexivitazion
  $x\ is\ equal\ to\ x\ \mapsto x\ is\ equal\ to\ itself$
\item Modification
  $x\ is\ a\ number\ and\ x\ is\ even\ \mapsto x\ is\ an\ even\ number$
\end{itemize}

Scaling this to cover more phenomena, such as those from Ganesalingam's analysis
will pose challenges. Extending this work in general without very sophisticated
statistical methods is impossible because mathematicians will speak uniquely,
and so choosing how to extend a grammar that covers the multiplicity of ways of
saying ``the same thing" will require many choices and a significant corpus of
examples. The most interesting linguistic phenomena covered by this grammar,
In-situ quantification, has been at the heart of the Montague tradition.

While this grammar serves as a precedent for this work generally, we note that
the core logic only supports propositions without proofs - it is not a type
theory with terms. Additionally, the domain of arithmetic is an important case
study, but scaling this grammar (or any other, for that matter) to allow for
\emph{semantic adequacy} of real mathematics is still far away, or as Ranta
concedes, ``it seems that text generation involves undecidable optimization
problems that have no ultimate automatic solution."

\subsubsection{A Question Answering Example}

We wrote a smaller version of the above grammar \cite{warrickQA} just focused on
propositional logic. It included an added component not just translating between
ASTs, but also allowing intermediary computation and of logical
propositions and numerical expressions.

After a Haskell evaluation of propositional expressions to their Boolean values,
which could possibly in the future be extended to predicate logic, the system
allows a question answering system which gave different kinds of answers - the
binary valued answer, the most verbose possible answer, and the answer which was
deemed the most semantically adequate, \codeword{Simple}, \codeword{Verbose},
and \codeword{Compressed}, respectively. All answers are technically syntactically
complete. An example question with the answers follows:
\begin{verbatim}
is it the case that if the sum of 3 , 4 and 5 is prime , odd and even then 4
  is prime and even

  Simple : yes .
  Verbose : yes . if the sum of 3 and the sum of 4 and 5 is prime and the sum
    of 3 and the sum of 4 and 5 is odd and the sum of 3 and the sum of 4 and
    5 is even then 4 is prime and 4 is even .
  Compressed : yes . if the sum of 3 , 4 and 5 is prime , odd and even then
    4 is prime and even .
\end{verbatim}

The extended grammar in this case only had lists of propositions and predicates,
and so it was much simpler than Ranta's Cade grammar. GF list categories are
then transformed into Haskell lists via the embedding, using GF list syntax
explicitly is necessary as it is tied to its external behavior as well. The
functions of interest for our discussion are:
\begin{verbatim}
  IsNumProp : NumPred -> Object -> Prop ;
  LstNumPred : Conj -> [NumPred] -> NumPred ;
  LstProp : Conj  -> [Prop] -> Prop ;
\end{verbatim}

Note that a numerical predicate, \codeword{NumPred}, represents, for instance,
primality. In order for our pipeline to answer the question, we had to not only
do transform trees, $\llbracket - \rrbracket : \{AST\} \rightarrow
\{AST\}$ , but also evaluate them in more classical domains $\llbracket -
\rrbracket : \{AST\} \rightarrow \mathds{N}$ for the arithmetic objects. The
boolean semantics, 
$\llbracket - \rrbracket : \{AST\} \rightarrow \mathds{B}$,
are called \codeword{evalProp} in Haskell.

The extension adds more complex cases to cover when evaluating propositions,
because a normal ``propositional evaluator" doesn't have to deal with lists. For
the most part, this evaluation is able to just apply Boolean semantics to the
\emph{canonical} propositional constructors, like \codeword{GNot}. 
However, one had to dig deeper inside \codeword{GIsNumProp} in order to resolve
bugs in lists.

\begin{verbatim}
evalProp :: GProp -> Bool
evalProp p = case p of
  ...
  GNot p -> not (evalProp p)
  ...
  GIsNumProp (GLstNumProp c (GListNumPred (x : xs))) obj ->
    let xo = evalProp (GIsNumProp x obj)
        xso = evalProp (GIsNumProp (GLstNumProp c (GListNumPred (xs))) obj) in
    case c of
      GAnd -> (&&) xo xso
      GOr -> (||) xo xso
  ...
\end{verbatim}
While this case is still relatively simple, an even more expressive abstract syntax
may yield many more subtle obstacles, which is the reason it's so hard
to understand PGF helper functions by just trying to read the code. The more semantic content one
incorporates into the GF grammar, the larger the PGF GADT, which leads to many
more cases when evaluating these trees.

\paragraph{Testing Anecdotes}

There were many obstructions in engineering this relatively simple example,
particularly when it came to writing test cases. The naive way to test with GF
is to translate, and the linearization and parsing functions don't give the
programmer many degrees of freedom. ASTs are not objects amenable to human
intuition, which makes it problematic because understanding their
transformations constantly requires parsing and linearizing to see their
``behavior". While some work has been done to allow testing of GF grammars for
NL applications \cite{listenmaa2019phd}, the domain of formal languages in GF
requires a more refined notion of testing because most utterances should be
testable relative to some model with well behaved mathematical properties.
Debugging something in the pipeline $String \rightarrow GADT \rightarrow GADT
\rightarrow String$ for a large scale grammar without a testing methodology for
each intermediate state is surely to be avoided.

Unfortunately, there is no published work on using Quickcheck \cite{quickCheck}
with the PGF library. The bugs in this grammar were discovered via the input and
output \emph{appearance} of strings. Often, no string would be returned after a
small change, and discovering if the source was in the abstract, concrete, or
Haskell embedding was excruciating. In one case, a bug was discovered that was
presumed to be from the PGF evaluator, but was then back-traced to Ranta's
grammar from which the code had been refactored. The sentence which broke our
pipeline from core to extended, "4 is prime , 5 is even and if 6 is odd then 7
is even", would be easily generated (or at least its AST) by Quickcheck.

\paragraph{Theorems and Linguistic Utterances}

An important observation that was made during this development : that theorems
should be the source of inspirations for deciding which PGF transformations
should take place. For instance, one could define $odd : \mathds{N} \rightarrow
Set$, $prime : \mathds{N} \rightarrow Set$ and prove that $\forall n \in
\mathds{N}.\; n > 2 \times prime\; n \implies odd\; n$. We can use this theorem
as a source of translation, and in fact encode a PGF rule that transforms
anything of the form ``n is prime and n is odd" to ``n is prime", subject to the
condition that $n \neq 2$. One could then take a whole set of theorems from
predicate calculus and encode them as Haskell functions which minify the
expressions to something with equivalent meaning. The verbose ``if $a$ then $b$
and if $a$ then $c$", can be more canonically read as ``if $a$ then $b$ and $c$".
The application of these theorems as evaluation functions in Haskell could help
give our QA example more informative and direct answers.

\paragraph{Reflections on Grammar Refactoring} One of the difficulties
encountered in this work was reverse engineering Ranta's GF and Haskell code-
the large size and declarative nature of a grammar makes it incredibly difficult
to isolate individual features one may wish to understand. Significant efforts
went into filtering the grammars to understand behaviors of individual
components. Careful usage of the GF module system may sometimes allow one to
look at ``subgrammars", but there is not proper methodology to extract a
sub-grammar and therefore it was found that writing a grammar from scratch was
often the easiest way to do this. While grammars can be written compositionally,
decomposing them is often not a compositional process.
