\section{Previous Work}

According to legend, Göran Sundholm and Per Martin-Löf were sitting at a dinner
table, discussing various questions of interest, and Sundholm presented
Martin-Löf with the problem of \emph{donkey sentences} in natural language
semantics, statements analogous ``Every man who owns a donkey beats it". This
had been puzzling to those in the Montague tradition, whereby higher order logic
didn't provide facile ways of interpreting these sentences. Martin-Löf
apparently then, using dependent types, provided an interpretation of the donkey
sentence on the back of the napkin. This is perhaps the genesis of dependent
type theory in natural language semantics. The research program was thereafter
taken up by Martin-Löf's student Aarne Ranta \cite{ranta1994type}, bled into the
development of GF, and has now led to this current work.

The prior exploration of formal languages for the interleaving Trinitarian
subjects, is vast, and we can only sample the literature\cite{surveyLang}. Our
approach, using GF ASTs as a has many roots and interconnections with this
literature. The success of finding a suitable language for mathematics will
obviously require a comparative analysis of the strengths and weaknesses in such
a vast bibliography. How the GF approach compares with this long list merits
careful consideration and future work. We focus on a few resources.

\subsection{Ranta}

The initial considerations of Ranta were both oriented towards the language of
mathematics \cite{ranta93}, as well as purely linguistic concerns
\cite{ranta1994type}. In his treatise, Ranta explores not just the many avenues
to describe NL semantic phenomena with dependent types, but, after concentrating
on a linguistic analysis, he also proposes a primitive way of parsing and
sugaring these dependently typed interpretations of utterances into the strings
themselves - introducing the common nouns as types idea which has been since
seen great interest from both type theoretic and linguistic communities
\cite{luoCNs}. Therefore, if we interpret the set of men and the set of donkeys
as types, e.g. we judge $\vdash man \; {:} {\rm type}$ and $\vdash donkey \; {:}
{\rm type}$ where ${\rm type}$ really denotes a universe, and ditransitive verbs
``owns'' and ``beats'' as predicates, or dependent types over the CN types, i.e.
$\vdash owns \; {:} man \rightarrow donkey \rightarrow {\rm type}$ we can
interpret the sentence ``every man who owns a donkey beats it'' in DTT via the
following judgment :

\[\Pi z : (\Sigma x : man. \; \Sigma y : donkey. \; owns(x,y)). \; beats(\pi_1z,\pi_1(\pi_2z))\]

We note that the natural language quantifiers, which were largely the subject of
Montague's original investigations \cite{Montague1973}, find a natural
interpretation as the dependent product and sum types, $\Pi$ and $\Sigma$,
respectively. As type theory is constructive, and requires explicit witnesses
for claims, we admit the following semantic interpretation : given a man $m$, a
donkey $d$ and evidence $m-owns-d$ that the man owns the donkey, we can supply,
via the term of the above type applied to our own tripple $(m,d,m-owns-d)$ ,
evidence that the man beats the donkey, $beats(m,d)$ via the projections $\pi_1$
and $\pi_2$, or $\Sigma$ eliminators.

In the final chapter of \cite{ranta1994type}, \emph{Sugaring and Parsing}, Ranta
explores the explicit relation, and of translation between the above logical
form and the string, where he presents a GF predecessor in the Alfa proof
assistant, itself a predecessor of Agda. To accomplish this translation he
introduces an intermediary , a functional phrase structure tree, which later
becomes the basis for GF's abstract syntax.  What is referred to as ``sugaring''
later changes to ``linearization''.

Soon thereafter, GF became a fully realized vision, with better and more
expressive parsing algorithms \cite{ljunglof2004expressivity} developed in
Göteborg allowed for sugaring that can largely accommodate morphological
features of the target natural language \cite{rantaForsberg}, the translation
between the functional phrase structure (ASTs) and strings \cite{ranta_2004}.

Interestingly, the functions that were called $ambiguation : MLTT \to \{Phrase\
Structure\}$ and $interpretation : \{Phrase Structure\} \to MLTT$ were absorbed
into GF by providing dependently typed ASTs, which allows GF not just to parse
\emph{syntactic} strings, but only parse semantically well formed, or meaningful
strings. Although this feature was in some sense the genesis that allowed GF to
implement the lingusitic ideas from the book \cite{rantaTT}, it has remained
relatively untouched on the GF programmers tool-belt. Nonetheless, it was
intriguing enough to investigate briefly during the course of this work as one
can implement a programming language grammar that only accepts well typed
programs, at least as far as they can be encoded via GF's dependent types
\cite{warrickHarper}. Although GF isn't explicitly designed with type-checking
in mind , it would be very interesting to apply GF dependent types in the more
advanced programming languages to filter meaningless strings.

While the semantics of natural language in MLTT is relevant historically, it is
not the focus of this thesis. Its relevance comes from the fact that all these
ideas were circulating in the same circles - that is, Ranta's writings on the
language of mathematics, his approach to NL semantics, the derivative
development of GF and their confluence. This led to the development of a natural
language layer to Alfa \cite{alfaGF}, which can be seen as a direct predecessor
to this work. In some sense, we seek to recapitulate what was already done in
1998 - but this was prior to both GF's completion, and Alfa's hard fork to Agda.

\subsection{Mohan Ganesalingam}

\begin{displayquote}
There is a considerable gap between what mathematicians claim is true and what
they believe, and this mismatch causes a number of serious linguistic problems. 
\emph{Mohan Ganesalingam} \cite{ganesalingam2013language}
\end{displayquote}

The most substantial analysis of the linguistic perspective on written
mathematics comes from Ganesalingam \cite{ganesalingam2013language}. Not only
does he pick up and reexamine much of Ranta's early work, but he develops a
whole theory for how to understand the language mathematics from a formal point
of view, additionally working with many questions about the foundation of
mathematics. His model, which is developed early in the treatise and is
referenced throughout uses Discourse Representation Theory (DRT)
\cite{kamp2011discourse}, to capture anaphoric use of mathematical variables.
While he is interested in analyzing language, our goal is to translate, because
the meaning of an expression is contained in its set of formalizations. Our
project should be thought of as more of a way to implement the linguistic
features of mathematics rather than Ganesalingam's work focusing on analysis.

Ganesalingam draws insightful, nuanced conclusions from compelling examples.
Nonetheless, this subject is somewhat restricted to a specific linguistic
tradition and modern, textual mathematics. Therefore, we hope to 
contrast our GF point of view while offering some perspectives on
his work.

He remarks that mathematicians believe ``insufficiently precise" mathematical
sentences are those which would be result from a failure to translate them into
logic. This is much more true from the Agda developers perspective than the
mathematicians. Mathematicians generally assume small mistakes may go unnoticed
by the reviewers.

Ganesalingam also articulates ``mathematics has a normative notion of what its
content should look like; there is no analogue in natural languages." While this
is certainly true in \emph{local} cases surrounding a given mathematical
community , there are also many disputes - the Brouwer school is one example,
but our prior discussion of visual proofs also offers another counterexample.
Additionally, the GF perspective presented here is meant to disrupt the
notion of normativity, by suggesting that concrete syntax can reflect deep
differences in content beyond just its appearance.

He also discusses the important distinction between formal (which he focuses on)
and informal modes in mathematics, with the informal representing the
``commentary" which is assumed to be inexpressible in logic. GF, fortunately can
actually accommodate both if one considers only natural language translation in
the informal case. This is interesting because one would need extend a ``formal
grammar" with the general natural language content needed to include the
informal, although it is uncertain if the commentary should just be delegated to
comments if translated to Agda.

He says symbols serve to ``abbreviate material", and ``occur inside textual
mathematics". While his discourse records can deal with symbols, in GF,
overloading of symbols can cause overgeneration. For example certain words like
"is" and "are" can easily be interpreted as equality, equivalence, or
isomorphism depending on the context.

One of Ganesalingam's original contributions is the notion of adaptivity :
``Mathematical language expands as more mathematics is encountered". He
references some person's various stages of coming to terms with concepts in
mathematics and their generalization in that person's head. For instance, one
can define the concept of the $n$ squared as $n^2$ of two as ``$n$*$n$", which
are definitionally equal in Agda if one is careful about how one defines
addition, multiplication, and exponentiation, but require proof otherwise.
These details are unaccounted for by the mathematician, but can often hamper
formalization efforts because the substitution of propositionally equal and
definitionally equal terms are treated differently outside of select type theories.

Mathematical variables, it is also noticed, can be treated anaphorically. From
the PL perspective they are just expressions. Creating a suitable translation
from textual math to formal languages accounting for anaphora with GF proves to
be exceedingly tricky, as can be seen in the HoTT grammar below. These examples
should showcase that Ganesalingam's analysis, while certainly helpful when
building grammars, may require additional analysis to actually make things work.

\subsubsection{Pragmatics in mathematics}

Ganesalingam makes one observation which is particularly pertinent to our
analysis and understanding of mathematical language, which is that of pragmatics
content. The point warranted both a rebuttal \cite{RUFFINO2020114} and an
additional response by Ranta \cite{RANTA2020120}. Ganesalingam says
``mathematics does not exhibit any pragmatic phenomena: the meaning of a
mathematical sentence is merely its compositionally determined semantic content,
and nothing more."

San Mauro et al. disagree with this conception, stating mathematicians may rely
``on rhetorical figures, and speak metaphorically or even ironically", and that
mathematicians may forego literal meaning if considered fruitful. The authors
then give two technical examples of pragmatic phenomena where pragmatics is
explicitly exhibited, but we elect to give our own example relevant for our position on
the matter.

We look at the difference in meaning between lemma, proof, and corollary. While
there is a syntactic distinction between \term{Lemma} and \term{Theorem} in Coq,
Agda, which resembles Haskell rather than a theorem prover , sees no distinction
as seen in \autoref{fig:O1}. The words carry semantic weight : \emph{lemma} for
concepts preceding theorems and \emph{corollaries} for concepts applying
theorems. The interpretation of the meaning when a lemma or corollary is called
carry pragmatic content in that the author has to decide - how to judge the
content by its ``importance" and its relation to the \emph{theorems}. Inferring
how to judge a keyword seems impossible for a machine, especially since critical
results are misnamed - the Yoneda Lemma is just one of many examples.

Ranta categorizes pragmatic phenomena in 5 ways : speech acts, context,
speaker's meaning, efficient communication, and the \emph{wastebasket}. He
asserts that the disagreement is really a matter of how coarsely pragmatics is
interpreted by the authors - Ganesalingam applies a very fine filter in his
study of mathematical language, whereas the coarser filter applied by San Mauro
et al. allows for many more pragmatics phenomena to be captured and that the
``wastebasket" category is really the application of this filter. Ranta shows
that both speech acts and context are pragmatic phenomena treated in
Ganesalingam's work and speaker's meaning and efficient communication are
covered by San Mauro et al. Ranta contends that the authors disagreement arises
less about the content itself and how it is analyzed, but rather whether the
analysis should be classified as pragmatic or semantic.

Our grammars give us tools to study the \emph{speaker's meaning} of a
mathematical utterance by trying to translate them into syntactically complete
Agda judgments. \emph{Efficient communication} is the goal of producing a
semantically adequate grammar. The prospect of creating a grammar which
satisfies both is the most difficult task. We therefore hope that modeling of
natural language mathematics will give insights into how understanding of all
five pragmatic phenomena are necessary for worthwhile translations between CNLs
and formal languages, even if our grammars only really work with syntax. For the
CNLs to really be ``natural", one must be able to infer and incorporate
pragmatic phenomena.

Ganesalingam points out that ``a disparity between the way we think about
mathematical objects and the way they are formally defined causes our linguistic
theories to make incorrect predictions." This constraint on our theoretical
understanding of language, and the practical implications yield a bleak outlook.
Nevertheless, mathematical objects developing over time is natural, the more and
deeper we dig into the ground, the more we develop refinements of what kind of
tools we are using, develop better iterations of the same tools (or possibly
entirely new ones, and additionally learn about the soil in which we are
digging.

\subsection{Other authors}

\begin{displayquote}
QED is the very tentative title of a project to build a computer system that effectively represents all important mathematical knowledge and techniques.
\cite{godel1994qed} 
\end{displayquote}

The ambition of the QED Manifesto, with formalization and informalization of
mathematics being a subset, is probably impossible. The myriad attempts
at formalization and informalization are too much to compress here - a survey
and comparison of these ideas is unfortunately unavailable. We recount some of
them briefly.

The Naproche project (Natural language Proof Checking) is a CNL for studying the
language of mathematics by using Proof Representation Structures, a mutated form
of Discourse Representation Structures \cite{cramer2009naproche}. A central
goal of Naproche is to develop a controlled natural language (CNL), based off FOL, for
mathematics texts. It parses a theorem from the CNL into fully formal
statement, and then comes with a proof checking back-end to allow verification,
where it uses an Automated Theorem Prover (ATP) to check for correctness.
While the language is quite ``natural looking", it doesn't offer the same
linguistic flexibility as our GF approach and aspirations.

Mizar is a system attempting to be a formal language,which mathematicians can
use to express their results, and a database \cite{rudnicki1992overview}. It is
based off Tarski-Grothendeick set theory, and allows for correctness checking of
articles. It was originally developed concurrent to Martin-Löf's work in 1973,
and so much of the interest in types instead of sets couldn't be anticipated.
The focus Mizar on syntax resembling mathematics was pioneering, nonetheless, it
uses clumsy references and looks unreadable to those without expertise. Mizar
has a journal devoted to results in it, \emph{Formalized Mathematics}, and
offers a large library of known results. Additionally, it has inspired
iterations for other vernacular proof assistants, like Isabelle's Intelligible semi-automated
reasoning (Isar) extension \cite{wenzel2004isabelle}.

Subsequently, in \cite{mlTrans}, the authors take a corpus of parallel Mizar proofs natural
language proofs with latex, and seek to \emph{autoformalize} natural language
text with the intention of, in the future, further elaboration into an ITP.
This work uses traditional language models from the machine learning community, and analyze
the results. They were able to see some results, but nothing that as of yet can
be foreseen to general use.  Interestingly, a type elaboration mechanism in some
of their models was shown to bolster results.

Formalization seems more feasible with machine learning methods than
informalization , partially because tactics like ``hammer" in Coq for example,
are capable of some fairly large proofs \cite{czajka2018hammer} . Nonetheless,
for the Agda developer this isn't yet very relevant, and it's debatable whether
it would even be desirable. Voevodsky, for example, was apparently skeptical of
the usefulness of automated theorem proving for much of mathematics, as are many
mathematicians (although this is certainly changing).

The Boxer system, a CCG parser \cite{bos-etal-2004-wide} which allows English
text translation into FOL. However, it is not always correct, and dealing with
the language of mathematics will present obstacles. 

In \cite{proofFrom} the authors test the informalization. Despite working with
Coq, the the authors poignantly distinguish between proof scripts, sequences of
tactics, and proof objects, and focus on natural deduction proofs. Since Coq is
equipped with notions of Set, Type, and Prop, their methods make distinguishing
between these possibly easier. This work only focuses on linearization of trees,
and GF's pretty printer is likely superior to any NL generation techniques
because of help from the Resource Grammar Library (RGL). The complexity of the system also made it
untenable for larger proofs - nonetheless, it serves as an important prelude to
many of the subsequent GF developments in this area.

There are many other examples worth exploring in the natural language and
theorem prover boundary. It should be noted that GF's role in this space is
primitive, but it does offer the advantage of providing interface for natural
languages and programming languages. We also hope other PL developers will and
use and develop tools like the Grammatical Logical Inference Framework (GLIF),
which uses GF as a front-end for the Meta-Meta-Theory framework
\cite{schaefer2020glif}. With many approaches are not mentioned here, we a
hungry reader should evaluate these many sources with respect to this work.