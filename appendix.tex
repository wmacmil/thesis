\section{Appendix} \label{appendix}


\subsection{Martin-Löf Type Theory} \label{judge}

\subsubsection{Judgments}

\begin{displayquote}
With Kant, something important happened, namely, that the term judgement, Ger.
Urteil, came to be used instead of proposition. \emph{Per Martin-Löf} \cite{mlMeanings}.
\end{displayquote}

A central contribution of Per Martin-Löf in the development of type theory was
the recognition of the centrality of judgments in logic. Many mathematicians
aren't familiar with the spectrum of judgments available, and merely believe
they are concerned with \emph{the} notion of truth, namely \emph{the truth} of a
mathematical proposition or theorem. There are many judgments one can make which
most mathematicians aren't aware of or at least never mention. Examples of both familiar
and unfamiliar judgments include,

\begin{itemize}

\item $A$ is true
\item $A$ is a proposition
\item $A$ is possible
\item $A$ is necessarily true
\item $A$ is true at time $t$

\end{itemize}

These judgments are understood not in the object language in which we state our
propositions, possibilities, or probabilities, but as assertions in the
metalanguage which require evidence for us to know and believe them. Most
mathematicians may reach for their wallets if I come in and give a talk saying
it is possible that the Riemann Hypothesis is true, partially because they
already know that, and partially because it doesn't seem particularly
interesting to say that something is possible, in the same way that a physicist
may flinch if you say alchemy is possible. Most mathematicians, however, would
agree that $P = NP$ is a proposition, and it is also possible, but isn't true.

For the logician these judgments may well be interesting because their may be
logics in which the discussion of possibility or necessity is even more
interesting than the discussion of truth. And for the type theorist interested
in designing and building programming languages over many various logics, these
judgments become a prime focus. The role of the type-checker in a programming
language is to present evidence for, or decide the validity of the judgments.
The four main judgments of type theory are given in natural language on the left
and symbolically on the right :

\begin{multicols}{2}
\begin{itemize}
\item $T$ is a type
\item $T$ and $T'$ are equal types
\item $t$ is a term of type $T$
\item $t$ and $t'$ are equal terms of type $T$
\item $\vdash T \; {\rm type}$
\item $\vdash T = T'$
\item $\vdash t:T$
\item $\vdash t = t':T$
\end{itemize}
\end{multicols}

Frege's turnstile, $\vdash$, denotes a judgment. These judgments become much more interesting when we add the ability for them to
be interpreted in a some context with judgment hypotheses. Given a series of
judgments $J_1,...,J_n$, denoted $\Gamma$, where $J_i$ can depend on previously
listed $J's$, we can make judgment $J$ under the hypotheses, e.g. $J_1,...,J_n
\vdash J$. Often these hypotheses $J_i$, alternatively called \emph{antecedents},
denote variables which may occur freely in the *consequent* judgment $J$. For
instance, the antecedent, $x : \mathbb{R}$ occurs freely in the syntactic
expression $\sin x$, a which is given meaning in the judgment $\vdash \sin x { :
} \mathbb{R}$. We write our hypothetical judgement as follows :

$$x : \mathbb{R} \vdash \sin x : \mathbb{R}$$



\subsubsection{Rules}

Martin-Löf systematically used the four fundamental judgments in the proof
theoretic style of Pragwitz and Gentzen. To this end, the intuitionistic formulation of the
logical connectives just gives rules which admit an immediate computational
interpretation. The main types of rules are type formation, introduction,
elimination, and computation rules. The introduction rules for a type admit an
induction principle derivable from that type's signature. Additionally, the
$\beta$ and $\eta$ computation rules are derivable via the composition of
introduction and elimination rules, which, if correctly formulated, should
satisfy a relation known as harmony.

The fundamental notion of the lambda calculus, the function, is 
abstracted over a variable and returns a term of some type when applied to an
argument which is subsequently reduced via the computational rules.
Dependent Type Theory (DTT) generalizes this to allow the return type be
parameterized by the variable being abstracted over. The dependent function
forms the basis of the LF which underlies Agda and GF. Here is the formation
rule : 

\[
  \begin{prooftree}
    \hypo{̌\Gamma  \vdash A \; {\rm type}}
    \hypo{̌\Gamma, x : A \vdash B \; {\rm type}}
    \infer2[]{\Gamma \vdash \Pi x {:} A. B}
  \end{prooftree}
\]

One reason why hypothetical judgments are so interesting is we can devise rules
which allow us to translate from the metalanguage to the object language using
lambda expressions. These play the role of a function in mathematics and
implication in logic. More generally, this is a dependent type, representing the
$\forall$ quantifier. Assuming from now on $\Gamma \vdash A \; {\rm type}$ and
$\Gamma, x : A \vdash B \; {\rm type}$, we present here the introduction rule for
the most fundamental type in Agda, denoted \term{(x : A) -> B}.

\[
  \begin{prooftree}
    \hypo{̌\Gamma, x {:} A \vdash B \; {\rm type}}
    \infer2[]{\Gamma \vdash \lambda x. b {:} \Pi x {:} A. B}
  \end{prooftree}
\]

Observe that the hypothetical judgment with $x : A$ in the hypothesis has been
reduced to the same hypothesis set below the line, with the lambda term and Pi
type now accounting for the variable.

\[
  \begin{prooftree}
    \hypo{\Gamma \vdash f {:} \Pi x {:} A. B}
    \hypo{\Gamma \vdash a {:} A}
    \infer2[]{\Gamma \vdash f\, a {:} B[x := a]}
  \end{prooftree}
\]

We briefly give the elimination rule for
Pi, application, as well as the classic $\beta$ and $\eta$ computational equality
judgments (which are actually rules, but it is standard to forego the premises): 
$$\Gamma \vdash (\lambda x.b)\, a \equiv b[x := a] {:} B[x := a]$$
$$\Gamma \vdash (\lambda x.f)\, x \equiv f {:} \Pi x{:}A. B}$$
Using this rule, we now see a typical judgment without hypothesis in a real
analysis, $\vdash \lambda x. \sin x : \mathbb{R} \rightarrow \mathbb{R}$.  This is normally
expressed as follows (knowing full well that $\sin$ actually has to be
approximated when saying what the computable function in the codomain is): 
\begin{align*}
  \sin {:} \mathbb{R} &\rightarrow \mathbb{R}\\ x &\mapsto \sin ( x )
\end{align*}
Evaluating this function on 0, we see
\begin{align*}
(\lambda x. \sin x)\, 0 &\equiv \sin 0   \\ &\equiv 0
\end{align*}

While most mathematicians take this for granted, we hope this gives some insight
into how computer scientists present functions. We recommend reading
Martin-Löf's original papers \cite{ml1984} \cite{ml79} to see all the rules
elaborated in full detail, as well as his philosophical papers
\cite{mlMeanings} \cite{mlTruth} to understand type theory as it was conceived
both practically and philosophically.

% TODO : ADD stuff about substution, variable binding

\subsection{What is a Homomorphism?} \label{homo}

To get a feel for the syntactic paradigm we explore in this thesis, let us look at a basic mathematical
example: that of a group homomorphism as expressed in by a variety of somewhat
randomly sampled authors.  

% Wikipedia Defn:

\begin{definition}
In mathematics, given two groups, $(G, \ast)$ and $(H, \cdot)$, a group homomorphism from $(G, \ast)$ to $(H, \cdot)$ is a function $h : G \to H$ such that for all $u$ and $v$ in $G$ it holds that
  $$h(u \ast v) = h ( u ) \cdot h ( v )$$ 
\end{definition}

% http://math.mit.edu/~jwellens/Group%20Theory%20Forum.pdf

\begin{definition}
Let $G = (G,\cdot)$ and $G' = (G',\ast)$ be groups, and let $\phi : G \to G'$ be a map between them. We call $\phi$ a \textbf{homomorphism} if for every pair of elements $g, h \in G$, we have 
% \begin{center}
  $$\phi(g \ast h) = \phi ( g ) \cdot \phi ( h )$$ 
% \end{center}
\end{definition}

% http://www.maths.gla.ac.uk/~mwemyss/teaching/3alg1-7.pdf

\begin{definition}\label{def:def3}
Let $G$, $H$, be groups.  A map $\phi : G \to H$ is called a \emph{group homomorphism} if
  $$\phi(xy) = \phi ( x ) \phi ( y )\ for\ all\ x, y \in G$$ 
(Note that $xy$ on the left is formed using the group operation in $G$, whilst the product $\phi ( x ) \phi ( y )$ is formed using the group operation $H$.)
\end{definition}

% NLab:

\begin{definition}\label{def:def4}
Classically, a group is a monoid in which every element has an inverse (necessarily unique).
\end{definition}

We inquire the reader to pay attention to nuance and difference in presentation
that is normally ignored or taken for granted by the fluent mathematician, ask
which definitions feel better, and how the reader herself might present the
definition differently.

If one wants to distill the meaning of each of these presentations, there is a
significant amount of subliminal interpretation happening very much analogous to
our innate linguistic usage. The inverse and identity are discarded, even though
they are necessary data when defining a group. The order of presenting the
information is inconsistent, as well as the choice to use symbolic or natural
language information. In Definition~\ref{def:def3}, the group operation is used
implicitly, and its clarification a side remark. Details aside, these all mean
the same thing - don't they?


We now show yet another definition of a group homomorphism formalized in the
Agda programming language:

\input{latex/monoid}

While the last two definitions be somewhat compressible to a programmer
or mathematician not exposed to Agda, it is certainly comprehensible to a
computer : that is, the colors indicate it type-checks on a computer where Cubical Agda is installed.
While GF is designed for multilingual syntactic transformations and is targeted
for natural language translation, its underlying theory is largely based on
ideas from the compiler communities. A cousin of the BNF Converter (BNFC), GF is
fully capable of parsing programming languages like Agda! While the Agda
definitions are present another concrete presentation of a group
homomorphism, they are distinct in that they have inherent semantic content.

While this example may not exemplify the power of Agda's type-checker, it is of
considerable interest to many. The type-checker has merely assured us that
\term{GroupHom(')} are well-formed types - not that we have a canonical representation
of a group homomorphism.
% Aarne says cut

We note that the natural
language definitions of monoid differ in form, but also in pragmatic content.
How one expresses formalities in natural language is incredibly diverse, and
Definition~\ref{def:def4} as compared with the prior homomorphism definitions is
particularly poignant in demonstrating this. These differ very much in nature to
the Agda definitions - especially pragmatically.
The differences between the Cubical
Agda definitions may be loosely called pragmatic, in the sense that the choice
of definitions may have downstream effects on readability, maintainability, modularity, and other
considerations when trying to write good code, in a burgeoning area known as proof engineering.

% TODO section name


\input{latex/twinsigma}

\subsection{cubicalTT} \label{cubicaltt}
\begin{verbatim}
abstract Exp = {

flags startcat = Decl ;
      -- note, cubical tt doesn't support inductive families, and therefore the datatype (& labels) need to be modified

cat
  Comment ;
  Module  ;
  AIdent ;
  Imp ; --imports, add later
  Decl ;
  Exp ;
  ExpWhere ;
  Tele ;
  Branch ;
  PTele ;
  Label ;
  [AIdent]{0} ; -- "x y z"
  [Decl]{1} ;
  [Tele]{0} ;
  [Branch]{1} ;
  [Label]{1} ;
  [PTele]{1} ;
  -- [Exp]{1};

fun

  DeclDef : AIdent -> [Tele] -> Exp -> ExpWhere -> Decl ;
  -- foo ( b : bool ) : bool = b
  DeclData : AIdent -> [Tele] -> [Label] -> Decl ;
  -- data nat : Set where zero | suc ( n : nat )
  DeclSplit : AIdent -> [Tele] -> Exp -> [Branch] -> Decl ;
  -- caseBool ( x : Set ) ( y z : x ) : bool -> Set = split false -> y || true -> z
  DeclUndef : AIdent -> [Tele] -> Exp -> Decl ;
  -- funExt ( a : Set ) ( b : a -> Set ) ( f g : ( x : a ) -> b x ) ( p : ( x : a ) -> ( b x ) ( f x ) == ( g x ) ) : ( ( y : a ) -> b y ) f == g = undefined

  Where : Exp -> [Decl] -> ExpWhere ;
  -- foo ( b : bool ) : bool =
  -- f b where f : bool -> bool = negb
  NoWhere : Exp -> ExpWhere ;
  -- foo ( b : bool ) : bool =
  -- b

  Split : Exp -> [Branch] -> Exp ;
  --split@ ( nat -> bool ) with zero  -> true || suc n -> false
  Let : [Decl] -> Exp -> Exp ;
  -- foo ( b : bool ) : bool =
  -- let f : bool -> bool = negb in f b
  Lam : [PTele] -> Exp -> Exp ;
  -- \\ ( x : bool ) -> negb x
  -- todo, allow implicit typing
  Fun : Exp -> Exp -> Exp ;
  -- Set -> Set
  -- Set -> ( b : bool ) -> ( x : Set ) -> ( f x )
  Pi    : [PTele] -> Exp -> Exp ;
  --( f : bool -> Set ) -> ( b : bool ) -> ( x : Set ) -> ( f x )
  -- ( f : bool -> Set ) ( b : bool ) ( x : Set ) -> ( f x )
  Sigma : [PTele] -> Exp -> Exp ;
  -- ( f : bool -> Set ) ( b : bool ) ( x : Set ) * ( f x )
  App : Exp -> Exp -> Exp ;
  -- proj1 ( x , y )
  Id : Exp -> Exp -> Exp -> Exp ;
  -- Set bool == nat
  IdJ : Exp -> Exp -> Exp -> Exp -> Exp -> Exp ;
  -- J e d x y p
  Fst : Exp -> Exp ; -- "proj1 x"
  Snd : Exp -> Exp ; -- "proj2 x"
  -- Pair : Exp -> [Exp] -> Exp ;
  Pair : Exp -> Exp -> Exp ;
  -- ( x , y )
  Var : AIdent -> Exp ;
  -- x
  Univ : Exp ;
  -- Set
  Refl : AIdent ; -- Exp ;
  -- refl
  --Hole : HoleIdent -> Exp ; -- need to add holes

  OBranch :  AIdent -> [AIdent] -> ExpWhere -> Branch ;
  -- suc m -> split@ ( nat -> bool ) with zero -> false || suc n -> equalNat m n
  -- for splits

  OLabel : AIdent -> [Tele] -> Label ;
  -- suc ( n : nat )
  -- fora data types

  -- construct telescope
  TeleC : AIdent -> [AIdent] -> Exp -> Tele ;
  -- "( f g : ( x : a ) -> b x )"
  -- ( a : Set ) ( b : ( a ) -> ( Set ) ) ( f g : ( x : a ) -> ( ( b ) ( x ) ) ) ( p : ( x : a ) -> ( ( ( b ) ( x ) ) ( ( f ) ( x ) ) == ( ( g ) ( x ) ) ) )

  -- why does gr with this fail so epically?
  PTeleC : Exp -> Exp -> PTele ; 
  -- ( x : Set ) -- ( y : x -> Set )" -- ( x : f y z )"

  --everything below this is just strings

  Foo : AIdent ;
  A , B , C , D , E , F , G , H , I , J , K , L , M , N , O , P , Q , R , S , T , U , V , W , X , Y , Z : AIdent ;
  True , False , Bool : AIdent ;
  NegB : AIdent ;
  CaseBool : AIdent ;
  IndBool : AIdent ;
  FunExt : AIdent ;
  Nat : AIdent ;
  Zero : AIdent ;
  Suc : AIdent ;
  EqualNat : AIdent ;
  Unit : AIdent ;
  Top : AIdent ;
  Contr : AIdent ;
  Fiber : AIdent ;
  IsEquiv : AIdent ;
  IdIsEquiv : AIdent ;
  IdFun : AIdent ;
  ContrSingl : AIdent ;
  Equiv : AIdent ;
  EqToIso : AIdent ;
  Ybar : AIdent ;
  IdFib : AIdent ;
  Identity : AIdent ;
  Lemma0 : AIdent ;
}
\end{verbatim}
\begin{verbatim}
concrete ExpCubicalTT of Exp = open Prelude, FormalTwo in {

lincat 
  Comment,
  Module ,
  AIdent,
  Imp,
  Decl ,
  ExpWhere,
  Tele,
  Branch ,
  PTele,
  Label,
    -- = Str ;
  [AIdent],
  [Decl] ,
  -- [Exp],
  [Tele],
  [Branch] ,
  [PTele],
  [Label]
    -- = {hd,tl : Str} ;
    = Str ;
  Exp = TermPrec ;

lin

  DeclDef a lt e ew = a ++ lt ++ ":" ++ usePrec 0 e ++ "=" ++ ew ;
  DeclData a t d = "data" ++ a ++ t ++ ": Set where" ++ d ;
  DeclSplit ai lt e lb = ai ++ lt ++ ":" ++ usePrec 0 e ++ "= split" ++ lb ;
  DeclUndef a lt e = a ++ lt ++ ":" ++ usePrec 0 e ++ "= undefined" ; -- postulate in agda

  Where e ld = usePrec 0 e ++ "where" ++ ld ;
  NoWhere e = usePrec 0 e ;

  Let ld e = mkPrec 0 ("let" ++ ld ++ "in" ++ (usePrec 0 e)) ;
  Split e lb = mkPrec 0 ("split@" ++ usePrec 0 e ++ "with" ++ lb) ;
  Lam pt e = mkPrec 0 ("\\" ++ pt ++ "->" ++ usePrec 0 e) ;
  Fun = infixr 1 "->" ; -- A -> Set
  Pi pt e = mkPrec 1 (pt ++ "->" ++ usePrec 1 e) ;
  Sigma pt e = mkPrec 1 (pt ++ "*" ++ usePrec 1 e) ;
  App = infixl 2 "" ;
  Id e1 e2 e3 = mkPrec 3 (usePrec 4 e1 ++ usePrec 4 e2 ++ "==" ++ usePrec 3 e3) ;
-- for an explicit vs implicit use of parameters, may have to use expressions as records, with a parameter is_implicit
  IdJ e1 e2 e3 e4 e5 = mkPrec 3 ("J" ++ usePrec 4 e1 ++ usePrec 4 e2 ++ usePrec 4 e3 ++ usePrec 4 e4 ++ usePrec 4 e5) ;
  Fst e = mkPrec 4 ("fst" ++ usePrec 4 e) ;
  Snd e = mkPrec 4 ("snd" ++ usePrec 4 e) ;
  Pair e1 e2 = mkPrec 5 ("(" ++ usePrec 0 e1 ++ "," ++ usePrec 0 e2 ++ ")") ;
  Var a = constant a ;
  Univ = constant "Set" ;
  Refl = "refl" ; -- constant "refl" ;

  BaseAIdent = "" ;
  ConsAIdent x xs = x ++ xs ;

  -- [Decl] only used in ExpWhere
  BaseDecl x = x ;
  ConsDecl x xs = x ++ "^" ++ xs ;

  -- maybe accomodate so split on empty type just gives () 
  -- BaseBranch = "" ;
  BaseBranch x = x ;
  -- ConsBranch x xs = x ++ "\n" ++ xs ;
  ConsBranch x xs = x ++ "||" ++ xs ;

  -- for data constructors
  BaseLabel x = x ;
  ConsLabel x xs = x ++ "|" ++ xs ; 

  BasePTele x = x ;
  ConsPTele x xs = x ++ xs ;

  BaseTele = "" ;
  ConsTele x xs = x ++ xs ;

  OBranch a la ew = a ++ la ++ "->" ++ ew ;
  TeleC a la e = "(" ++ a ++ la ++ ":" ++ usePrec 0 e ++ ")" ;
  PTeleC e1 e2 = "(" ++ top e1 ++ ":" ++ top e2 ++ ")" ;

  OLabel a lt = a ++ lt ;

  --object language syntax, all variables for now
  Bool = "bool" ;
  True = "true" ;
  False = "false" ;
  CaseBool = "caseBool" ;
  IndBool = "indBool" ;
  FunExt = "funExt" ;
  Nat = "nat" ;
  Zero = "zero" ;
  Suc = "suc" ;
  EqualNat = "equalNat" ;
  Unit = "unit" ;
  Top = "top" ;
  Foo = "foo" ; 
  A = "a" ;
  B = "b" ;
  C = "c" ;
  D = "d" ;
  E = "e" ;
  F = "f" ;
  G = "g" ;
  H = "h" ;
  I = "i" ;
  J = "j" ;
  K = "k" ;
  L = "l" ;
  M = "m" ;
  N = "n" ;
  O = "o" ;
  P = "p" ;
  Q = "q" ;
  R = "r" ;
  S = "s" ;
  T = "t" ;
  U = "u" ;
  V = "v" ;
  W = "w" ;
  X = "x" ;
  Y = "y" ;
  Z = "z" ;
  NegB = "negb" ;
  -- everything below is for contractible proofs
  Contr = "isContr" ;
  Fiber = "fiber" ;
  IsEquiv = "isEquiv" ;
  IdIsEquiv = "idIsEquiv" ;
  IdFun = "idfun" ;
  ContrSingl = "contrSingl" ;
  Equiv = "equiv" ;
  EqToIso = "eqToIso" ;
  Identity = "id" ;
  Ybar = "ybar"  ;
  IdFib = "idFib"  ;
  Lemma0 = "lemma0" ;
}
\end{verbatim}
The resource FormalTwo.gf merely substitutes more precedences than Formal.gf
from the RGL, in the ideal case that we could scale the grammar to include
larger and more complicated fixity information.
\begin{verbatim}
resource FormalTwo = open Prelude in {

----Everything the same up until the definition of Prec in Formal.gf


    Prec : PType = Predef.Ints 9 ;

    highest = 9 ;

    lessPrec : Prec -> Prec -> Bool = \p,q ->
      case <<p,q> : Prec * Prec> of {
        <3,9> | <2,9> | <4,9> | <5,9> | <6,9> | <7,9> | <8,9> => True ;
        <3,8> | <2,8> | <4,8> | <5,8> | <6,8> | <7,8> => True ;
        <3,7> | <2,7> | <4,7> | <5,7> | <6,7> => True ;
        <3,6> | <2,6> | <4,6> | <5,6> => True ;
        <3,5> | <2,5> | <4,5> => True ;
        <3,4> | <2,3> | <2,4> => True ;
        <1,1> | <1,0> | <0,0> => False ;
        <1,_> | <0,_>         => True ;
        _ => False
      } ;

    nextPrec : Prec -> Prec = \p -> case <p : Prec> of {
      9 => 9 ;
      n => Predef.plus n 1
      } ;
\end{verbatim}


\input{latex/compare}

\subsection{HoTT Agda Corpus} \label{hottproofs}